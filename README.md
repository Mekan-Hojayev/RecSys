# RecSys
Recommender Systems + Project

Relative papers

| Paper | Description |
|-------|-------------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Introduces the Transformer model, which relies entirely on self-attention mechanisms without using sequence-aligned RNNs or convolution. |
| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) | Presents BERT, a transformer-based model that achieves state-of-the-art results on a wide range of NLP tasks by pre-training on large text corpora. |
| [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) | Proposes ResNet, a deep convolutional neural network architecture with residual connections that enables training of very deep networks. |
| [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) | Introduces GANs, a framework for training generative models by pitting two neural networks against each other: a generator and a discriminator. |
| [YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767) | Describes YOLOv3, a real-time object detection system that improves upon previous versions in terms of accuracy and speed. |
| [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) | Proposes EfficientNet, a family of models that achieve state-of-the-art accuracy with significantly fewer parameters and computational cost. |
| [AlphaGo: Mastering the Game of Go with Deep Neural Networks and Tree Search](https://www.nature.com/articles/nature16961) | Details AlphaGo, a program that combines deep neural networks with Monte Carlo tree search to defeat world champions in the game of Go. |
| [Mask R-CNN](https://arxiv.org/abs/1703.06870) | Presents Mask R-CNN, an extension of Faster R-CNN that adds a branch for predicting segmentation masks on each Region of Interest (RoI). |
| [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) | Introduces the Lottery Ticket Hypothesis, which suggests that dense, randomly-initialized networks contain smaller subnetworks that can train to similar performance. |
| [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) | Proposes an attention-based model for neural machine translation that learns to align and translate jointly. |
| [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) | Describes AlexNet, a deep convolutional neural network that significantly improved image classification accuracy on the ImageNet dataset. |
| [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) | Introduces DCGANs, a class of CNNs trained with adversarial loss for unsupervised representation learning. |
| [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948) | Presents StyleGAN, a generative adversarial network that generates high-quality images by controlling style at different levels of detail. |